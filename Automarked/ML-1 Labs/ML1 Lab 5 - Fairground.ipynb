{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairground\n",
    "\n",
    "This lab introduces _contextual multi-armed bandits_.\n",
    "It is also an opportunity to train some neural networks with PyTorch (an alternative to TensorFlow and Jax - it's good to have experience of multiple tools).\n",
    "\n",
    "The multi-armed bandit problem gets its name from the one armed bandit slot machine found in gambling dens (some of the earliest, very literally, had one arm: [one armed bandits](https://commons.wikimedia.org/wiki/File:One-Armed_Bandits_at_Stockmen%27s_Hotel,_Elko,_Nevada_(83581).jpg)).\n",
    "It gets the term \"_multi-armed bandit_\" because we imagine a row of them, all with different payout rates, i.e. if you pull the lever of the best your average reward will be higher than if you selected any of the other machines.\n",
    "The problem is we don't know which lever that is.\n",
    "\n",
    "This gives rise to the _exploration_ vs _exploitation_ problem, a core part of reinforcement learning (bandits are not an example of reinforcement learning, as they don't have sequential decision making, but they make for an excellent gateway algorithm).\n",
    "In this scenario we keep pulling arms, trying to maximise reward.\n",
    "At any given moment you can either choose to explore, i.e. try an arm to improve your estimate of its payout rate, or exploit, i.e. pull the arm you believe has the highest payout rate.\n",
    "Because there is noise in the payout you only ever have an estimate of the payout rate, e.g. it's entirely possible that a super rare event that substantially adjusts your estimates has yet to be observed, such as the jackpot.\n",
    "This means that at any given moment your estimate of which arm is best could be wrong, hence the need to explore never goes entirely away, though it does go down as you learn which arm is best.\n",
    "\n",
    "This problem may seem a little abstract at first, but it can be found in many places.\n",
    "A classic example is picking a website design, which is called _A/B testing_.\n",
    "You have two (or more) versions of a web page and want to know which is _best_, in some sense that you can measure.\n",
    "One way to approach this is as a classic scientific experiment: show version A for a month, then show version B for a month, then look at the measurement and choose the best.\n",
    "But that wastes two months, and doesn't help if you have twenty designs.\n",
    "Instead, a bandit will gather information and softly commit to a solution as the evidence it's the right choice grows.\n",
    "This same idea can be applied to the more serious problem of selecting which of several drugs is best for a condition.\n",
    "\n",
    "Back to the internet and an example you will have suffered is the intrusive displaying of adverts on websites.\n",
    "Each advert that could be displayed is a one armed bandit, and you want to place the advert with the highest payout, i.e. the one the user is most likely to click on.\n",
    "This introduces context, as we probably have some cookies that tell us something about the user, so we can select an advert specifically for them, rather than whichever one gets the highest global click through rate.\n",
    "The advert can also have an associated context.\n",
    "\n",
    "For this lab the scenario is a little more fun:\n",
    "you're coding a greeting robot at a fairground, so the job of the algorithm is to direct patreons to the ride that will make them happy.\n",
    "Both the patreons and rides have context.\n",
    "There are three questions that build up to a complete solution:\n",
    "\n",
    "1. Ignore context to introduce Thompson sampling as a solution for the basic multi-armed bandit problem\n",
    "2. Introduce PyTorch for training a neural network. This includes the context but to keep things simple there is no bandit; instead a neural network is trained once and then run, ignoring the exploration/exploitation problem entirely.\n",
    "3. All together: using a neural network to do Thompson sampling for a contextual multi-armed bandit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking\n",
    "\n",
    "This lab exercise has 10 marks to earn; they are formative and not actually worth anything towards your final grade.\n",
    "Every place you have to add code is indicated by\n",
    "\n",
    "`# **************************************************************** 2 marks`\n",
    "\n",
    "with instructions above the code block.\n",
    "\n",
    "You may submit your notebook to the auto marker [you.cs.bath.ac.uk](https://you.cs.bath.ac.uk) to get a grade.\n",
    "The notebook you submit must be an .ipynb file, which is saved into the directory you're running Jupyter; alternatively you can download it from the menu above using `File -> Download As -> Notebook (.ipynb)`.\n",
    "Remember to save your work regularly (`Save and checkpoint` in the `File` menu, the icon of a floppy disk, or `Ctrl-S`); the version you submit should have all code blocks showing the results (if any) of execution below them.\n",
    "It is wise to verify it runs to completion with _Restart & Run All_ before submission.\n",
    "\n",
    "You must comply with the universities plagiarism guidelines: http://www.bath.ac.uk/library/help/infoguides/plagiarism.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import fairground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below generates a random fairground and some random visitors\n",
    "It then assigns each random visitor to a random ride at the fairground and records the percentage that are happy.\n",
    "We can consider this to be the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairground contains:\n",
      "    Reverse bungee\n",
      "    Bungee trampolines\n",
      "    Mechanical bull\n",
      "    Go karts\n",
      "    Fun slide\n",
      "    Hayride\n",
      "    Helter skelter\n",
      "    Puny train ride\n",
      "    Arcade hall\n",
      "    Toboggan\n",
      "    Dodgems\n",
      "    Caterpillar\n",
      "    Planetarium\n",
      "    Rotor\n",
      "    Pub\n",
      "    Ferris wheel\n",
      "\n",
      "Random ride assignment:\n",
      "    29.9% of customers happy\n"
     ]
    }
   ],
   "source": [
    "rng = numpy.random.default_rng()\n",
    "\n",
    "\n",
    "# Generate a random fairground...\n",
    "rides, ride_indices = fairground.generate_fairground(rng)\n",
    "print('Fairground contains:')\n",
    "fairground.print_names(ride_indices, indent=4)\n",
    "print()\n",
    "\n",
    "\n",
    "# Generate some random visitors...\n",
    "visitors = fairground.patreons(1024 * 16, rng)\n",
    "\n",
    "\n",
    "# Try random assignment to rides, so we have a baseline level of happiness...\n",
    "print('Random ride assignment:')\n",
    "which = rng.choice(rides.shape[0], size=visitors.shape[0])\n",
    "\n",
    "assignment = numpy.empty((visitors.shape[0], 20))\n",
    "assignment[:,:10] = visitors\n",
    "assignment[:,10:] = rides[which,:]\n",
    "\n",
    "h = fairground.happy(assignment, rng)\n",
    "baseline = h.sum() / h.shape[0]\n",
    "\n",
    "print('    {:.1f}% of customers happy'.format(100 * baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Thompson sampling\n",
    "\n",
    "Thompson sampling is a Bayesian solution to the bandit problem.\n",
    "It's also pre-computer, dating from 1933, specifically the paper \"_[On the likelihood that one unknown probability exceeds another in view of the evidence of two samples](https://www.gwern.net/docs/statistics/decision/1933-thompson.pdf)_\" by Thompson.\n",
    "Definitely not recommended reading - language has changed dramatically since it was published.\n",
    "It's an example of a solution to a problem being forgotten, and then rediscovered many times when computers gave it a new reason to be relevant.\n",
    "Keep in mind that there are dozens of approaches to solving bandit problems, and while Thompson sampling is one of the best it's in practise not the best approach all of the time.\n",
    "It's one of the better ones however, and has convergence proofs.\n",
    "\n",
    "Thompson sampling is simple to explain, if not always easy to implement.\n",
    "Assuming you have a fully Bayesian model, that is you've got a PDF over model parameters, it consists of two steps:\n",
    "\n",
    "1. Draw a specific model instance (sample the posterior of the Bayesian model).\n",
    "2. Choose the best bandit according to the sampled model.\n",
    "3. Update the Bayesian model with the result of choosing that bandit.\n",
    "\n",
    "That's it!\n",
    "Or at least, that's a single step - you repeat this for each iteration.\n",
    "Because the distribution over model parameters represents our uncertainty the above selects each bandit proportional to it's probability of being the best.\n",
    "This naturally balances exploration and exploitation (noting that the phrase \"_naturally balances_\" is scientifically meaningless).\n",
    "\n",
    "For the fairground we'll presume that we score 0 if a patreon is unhappy and 1 if the patreon is happy.\n",
    "Remember that we are ignoring context for now, so the model can be a set of Bernoulli distributions, one per ride.\n",
    "The model is hence parameterised by the probability of each ride making a patreon happy.\n",
    "To make the model Bayesian we need a prior over these parameters - the Beta distribution is the obvious choice because it is the conjugate prior to the Bernoulli.\n",
    "This means we can start from a prior, specifically a $\\operatorname{\\beta eta}(1, 1)$ prior, as that's a uniform probability of each possible model values for each ride.\n",
    "The process is hence:\n",
    "\n",
    "1. Sample a happiness probability for each ride from the model (draw from Beta distributions).\n",
    "2. Pick the ride with the highest happiness probability for the current patreon.\n",
    "3. Observe if the patreon is happy and update the model (update Beta distribution parameters) accordingly.\n",
    "\n",
    "The below code already includes the update, in that it keeps track of how many times each ride has resulted in a happy vs sad patreon and hands it to a function you need to fill in.\n",
    "Your task is to code the function to return the index of the ride to send the patreon to, in compliance with Thompson sampling.\n",
    "\n",
    "Notes:\n",
    "* The `rng` parameter passed into `pick_thompson()` will be an instance of [numpy.random.Generator](https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator) - it has methods to generate random numbers from all of the standard PDFs.\n",
    "* This is very much an exercise in understanding the above - can be solved in two lines if you think it through! (and keep it vectorised)\n",
    "* Make sure you don't get $\\alpha$ and $\\beta$, the parameters of the $\\operatorname{\\beta eta}$ distribution, backwards!\n",
    "\n",
    "__(2 marks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thompson sampling ride assignment:\n",
      "    46.2% of customers happy\n",
      "\n",
      "Ride performance:\n",
      "      Reverse bungee: usage = 0.2%, ride happiness = 23.1%\n",
      "  Bungee trampolines: usage = 2.5%, ride happiness = 41.0%\n",
      "     Mechanical bull: usage = 14.8%, ride happiness = 46.8%\n",
      "            Go karts: usage = 4.3%, ride happiness = 43.3%\n",
      "           Fun slide: usage = 0.2%, ride happiness = 12.0%\n",
      "             Hayride: usage = 0.8%, ride happiness = 37.4%\n",
      "      Helter skelter: usage = 0.5%, ride happiness = 29.5%\n",
      "     Puny train ride: usage = 0.1%, ride happiness = 9.1%\n",
      "         Arcade hall: usage = 21.6%, ride happiness = 46.2%\n",
      "            Toboggan: usage = 0.5%, ride happiness = 26.7%\n",
      "             Dodgems: usage = 52.3%, ride happiness = 48.0%\n",
      "         Caterpillar: usage = 0.1%, ride happiness = 0.0%\n",
      "         Planetarium: usage = 0.2%, ride happiness = 16.1%\n",
      "               Rotor: usage = 0.8%, ride happiness = 36.5%\n",
      "                 Pub: usage = 1.0%, ride happiness = 36.4%\n",
      "        Ferris wheel: usage = 0.2%, ride happiness = 17.1%\n"
     ]
    }
   ],
   "source": [
    "def pick_thompson(sad, happy, rng):\n",
    "    \"\"\"This function is given two 1D arrays, indexed by ride, giving how many\n",
    "    times each ride has made a customer sad and how many times it has made a\n",
    "    customer happy. It returns the index of the ride to send a customer to.\n",
    "    Also provided with a rng for the sampling.\"\"\"\n",
    "\n",
    "    # **************************************************************** 2 marks\n",
    "    sample = rng.beta(happy+1,sad+1)\n",
    "    return numpy.argmax(sample)\n",
    "\n",
    "\n",
    "# Do a multi-armed bandit loop (not contextual) using the above pick_thompson() function...\n",
    "sad = numpy.zeros(rides.shape[0], dtype=int)\n",
    "happy = numpy.zeros(rides.shape[0], dtype=int)\n",
    "\n",
    "print('Thompson sampling ride assignment:')\n",
    "assignment = numpy.empty(20)\n",
    "\n",
    "for row in visitors:\n",
    "    choice = pick_thompson(sad, happy, rng)\n",
    "    \n",
    "    assignment[:10] = row\n",
    "    assignment[10:] = rides[choice,:]\n",
    "    \n",
    "    h = fairground.happy(assignment, rng)\n",
    "    (happy if h else sad)[choice] += 1\n",
    "\n",
    "\n",
    "# Print performance...\n",
    "thompson = happy.sum() / visitors.shape[0]\n",
    "print('    {:.1f}% of customers happy'.format(100 * thompson))\n",
    "print()\n",
    "\n",
    "\n",
    "# Print how many times each ride was selected and the happiness it contributed...\n",
    "print('Ride performance:')\n",
    "for i, fi in enumerate(ride_indices):\n",
    "    name = fairground.rides[fi]['name']\n",
    "    usage = 100 * (sad[i] + happy[i]) / visitors.shape[0]\n",
    "    happiness = 100 * happy[i] / numpy.maximum(sad[i] + happy[i], 1)\n",
    "    print(f'  {name:>18}: usage = {usage:.1f}%, ride happiness = {happiness:.1f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch quick start\n",
    "\n",
    "[PyTorch](https://pytorch.org) provides a simple workflow for generating neural networks within `torch.nn`.\n",
    "We will not be using it.\n",
    "Instead, we're going to implement the neural network at the same level as numpy, and only make use of the _automatic gradient_ calculation (autograd) for optimisation.\n",
    "This allows you to calculate the gradient of any parameter relative to your loss function, so you can perform gradient descent (or a better alternative).\n",
    "\n",
    "In Torch autograd is entirely explicit.\n",
    "Firstly, you create arrays using functions such as `torch.zeros()` - these are equivalent to `numpy` arrays but with the extra functionality needed for automatic differentiation.\n",
    "You then tell Torch which arrays you want gradients for; this can be done at construction\n",
    "```\n",
    "weights = torch.zeros(32, requires_grad=True)\n",
    "```\n",
    "or later\n",
    "```\n",
    "weights.requires_grad_()\n",
    "```\n",
    "Note that the underscore after the method call name indicates tht it is _in place_, that is it updates the weights array rather than returning a new array.\n",
    "Enabling it later is usually done because you need to fill in the array in with some complex calculation first, and don't want to waste memory preparing to calculate the gradient of that calculation.\n",
    "\n",
    "Once you have your arrays ready you can then do an arbitrary calculation on them (with `torch` only - if your calculation travels through `numpy` or traditional Python data structures then the connection will be broken and the gradient lost), which for machine learning will typically end up in the calculation of a `loss` that you want to minimise.\n",
    "You can then call `loss.backward()` on this variable.\n",
    "Every array for which you have enabled gradient calculation contains an additional parameter, `grad`, e.g. `weights.grad`.\n",
    "It's another array, same shape as its parent, and after calling `loss.backward()` it will contain the gradient of that array with respect to `loss`.\n",
    "This ability to calculate gradients automatically is the core of what makes PyTorch useful, and all we need to fit a neural network to data.\n",
    "\n",
    "Note that there is one extra detail that can bite you.\n",
    "`loss.backward()` doesn't replace the `.grad` arrays, instead it adds the gradient onto whatever is already in the `.grad` arrays.\n",
    "This is convenient, as you can use this to send multiple blocks of data through, updating the gradient after each block with `.backward()` before doing an optimisation step.\n",
    "But it does mean you must remember to zero out the gradients each time through the optimisation loop, e.g.\n",
    "```\n",
    "weights.grad.zero_()\n",
    "```\n",
    "Note again the use of an underscore to indicate it is inplace.\n",
    "\n",
    "Two final details.\n",
    "Sometimes you want to do some calculations without updating the gradients.\n",
    "The most common example of this is when performing the actual update of your model parameters as part of each optimisation step.\n",
    "This can be done with\n",
    "```\n",
    "with torch.no_grad():\n",
    "    weights -= weights.grad * lr\n",
    "```\n",
    "In this example we have a traditional gradient descent update with `lr` as the learning rate (step size).\n",
    "\n",
    "The second detail is that sometimes you want to keep a value around, when it was involved in gradient calculation, but without keeping the data structures required for gradient calculation.\n",
    "A good example of this is keeping the history of training loss as a network is trained.\n",
    "In fact, if you kept the computation graphs with the history the graph would get so big you would probably run out of memory!\n",
    "In such cases you can use `.detach()`:\n",
    "```\n",
    "history.append(loss.detach())\n",
    "```\n",
    "Note that, as a safety feature, you can't convert a tensor to a `numpy` array unless you detach it first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Neural network with dropout\n",
    "\n",
    "We're going to stick to a simple fully connected three layer neural network.\n",
    "All this means is we're learning a function from a vector to a single value\n",
    "\n",
    "$$y = f(\\mathbf{x})$$\n",
    "\n",
    "which happens in this case to take the form\n",
    "\n",
    "$$y = \\phi(\\phi(\\phi(\\mathbf{x}^T \\mathbf{W}_1 + \\mathbf{b}^T_1) \\mathbf{W}_2 + \\mathbf{b}^T_2) \\mathbf{W}_3 + \\mathbf{b}^T_3)$$\n",
    "\n",
    "where the $\\mathbf{W}$ are matrices of weights and the $\\mathbf{b}$ are column vectors of biases.\n",
    "Note that because the output is a single value technically $\\mathbf{W}_3$ is a vector and $\\mathbf{b}_3$ is a single value; matrix/vector notation is preserved so this is in the general form (i.e. same notation if $y$ is a vector), including in the data structures.\n",
    "Putting the input vector before the matrix happens to be convenient due to the way broadcasting works when it comes to the implementation.\n",
    "\n",
    "The $\\phi(\\cdot)$ function is the nonlinearity, which is always defined elementwise (i.e. if you give it a vector it will return a vector of the same size where the function has been applied independently to each element).\n",
    "In this case we're going to use the leaky ReLU (rectified linear unit), which can be defined as\n",
    "\n",
    "$$\\phi(v) = \\begin{cases} v & \\text{if}\\ v>0 \\\\ 0.01 v & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "$0.01$ is technically a parameter, but it's usually not worth tuning, and in this case should be left alone.\n",
    "\n",
    "To understand the relationship between the above equation and the typical diagram note that each $\\cdot \\mathbf{W} + \\mathbf{b}$ is the function for a linear transform representing the connections between layers and $\\phi(\\cdot)$ is the nonlinearity on each layer.\n",
    "These two functions are just being alternated between, with function composition, with each pair making up a layer of a neural network, that is then passed to the next.\n",
    "Writing it in terms of layer outputs, $\\mathbf{l}$, gives you\n",
    "\n",
    "$$\\mathbf{l}_0 = \\mathbf{x}^T$$\n",
    "\n",
    "$$\\mathbf{l}_n = \\phi(\\mathbf{l}_{n-1} \\mathbf{W}_n + \\mathbf{b}^T_n)$$\n",
    "\n",
    "\n",
    "\n",
    "### Dropout\n",
    "\n",
    "You'll note that Thompson sampling is dependent on having a Bayesian model, when the above neural network is not even probabilistic, let alone Bayesian.\n",
    "Dropout is a technique that gives us a probabilistic network.\n",
    "The original version consists of randomly dropping outputs from each layer during the training phase only (different draw for each exemplar), the idea being this improves robustness by preventing too much dependency on any single path through the network.\n",
    "You can extend this by also randomly dropping connections during evaluation.\n",
    "It has since been shown that this is, in a very specific sense, equivalent to a deep Gaussian process, with each noisy evaluation equivalent to a draw from the distribution.\n",
    "A deep GP is when you take multiple GPs and compose the functions; it's Bayesian, at least in the sense required for Thompson sampling.\n",
    "\n",
    "Modifying the three layer neural network equation to include dropout on just the first hidden layer it becomes\n",
    "\n",
    "$$y = \\phi(\\phi([\\phi(\\mathbf{x}^T \\mathbf{W}_1 + \\mathbf{b}^T_1) \\odot \\mathbf{k}^T_1] \\mathbf{W}_2 + \\mathbf{b}^T_2) \\mathbf{W}_3 + \\mathbf{b}^T_3)$$\n",
    "\n",
    "where $\\odot$ is the Hadamard product (element wise) and\n",
    "\n",
    "$$\\mathbf{k}_1 \\sim \\operatorname{Bernoulli}(1 - d)$$\n",
    "\n",
    "noting that the above is assumed to be vectorised, i.e. we do an independent draw from the Bernoulli distribution for each element of the vectors $\\mathbf{k}_1$.\n",
    "$d$ is the dropout rate, i.e. how often we lose an output from a layer.\n",
    "It's common to set it to a different value for each layer, but we're only going to have it on one layer here.\n",
    "\n",
    "Your task is to fill in the function `model()` below with the dropout version of a three layer NN with leaky ReLU.\n",
    "Note that a dictionary containing parameters has been provided for you.\n",
    "The input is a vector of length 20 (length of patreon feature vector + length of ride feature vector), the length of the first hidden layer should be of length 256 and the second hidden layer should be of length 128.\n",
    "Output is of course a single value.\n",
    "The dropout rate (probability of dropping a value) should be 0.25 for the first hidden layer only ($\\mathbf{k}_1$).\n",
    "You don't have dropout on the final layer, as randomly switching off the entire network would be silly.\n",
    "The other layers have been dropped as we don't need that much randomness for this to work and it saves computation (drawing from a pdf is expensive).\n",
    "\n",
    "Notes:\n",
    " * It must be properly vectorised, such that you can call `model()` with either a single vector or with a data matrix. This will naturally happen if you put the vector first. The `@` operator does matrix multiplication for the last two dimensions, broadcasted over the rest, automatically switching to vector-matrix multiplication as required. This includes `vector @ matrix` adding an implicit transpose. In other words it naturally does the right thing if you put the vector first. Broadcasting rules mean you don't have to transpose the bias either.\n",
    " * Broadcasting will add extra dimensions to the return value of `model()`. Return the [`.squeeze()`](https://pytorch.org/docs/stable/generated/torch.squeeze.html) of the calculated value, to remove those excess dimensions.\n",
    " * Torch can be very fussy about data types. If it complains that it needs a `Tensor` just convert, e.g. `torch.tensor(1.0)`\n",
    " * When using a library like PyTorch you should use the [random number generator](https://pytorch.org/docs/stable/torch.html#random-sampling) it provides. This is because this rng will also run on the GPU, which is important if you need the speed.\n",
    " * You may want to write your own leaky ReLU function (Torch does provide one, but where is the fun in using that?). Your choice though.\n",
    " * The auto marker won't give you a GPU when marking this, so while you can make the code use one if you want make sure it will gracefully degrade if one isn't available. It's certainly not needed.\n",
    "\n",
    "If you're desperate for further reading then dropout was proposed in \"_[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)_\" by Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov (2014).\n",
    "Doesn't really explain why.\n",
    "For the relationship between dropout and deep GPs see \"_[Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](http://proceedings.mlr.press/v48/gal16.pdf)_\" by Gal and Ghahramani (2016).\n",
    "Not an easy paper.\n",
    "The weight initialisation approach used below is described in \"_[Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)_\" by He, Zhang, Ren and Sun (2015).\n",
    "\n",
    "__(2 marks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero: tensor(0., grad_fn=<SqueezeBackward0>)\n",
      "Random: tensor(-0.0043, grad_fn=<SqueezeBackward0>)\n",
      "Block: tensor([-0.0004, -0.0033, -0.0044, -0.0045, -0.0032, -0.0013, -0.0002],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "##### First we define model parameters...\n",
    "# (I would normally put these in a class, but not everyone in the lab knows OOP hence doing\n",
    "#  it this way. Do not change because auto marker depends on the model() function calling structure!)\n",
    "theta = {}\n",
    "\n",
    "theta['w',1] = torch.empty((20, 256), requires_grad=True)\n",
    "theta['b',1] = torch.zeros(256, requires_grad=True)\n",
    "theta['w',2] = torch.empty((256, 128), requires_grad=True)\n",
    "theta['b',2] = torch.zeros(128, requires_grad=True)\n",
    "theta['w',3] = torch.empty((128, 1), requires_grad=True)\n",
    "theta['b',3] = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "\n",
    "# We need to initialise model parameters - we can't just set them to zero as that doesn't work\n",
    "# because optimisation will get stuck on ties (values will move in lockstep; doesn't happen \n",
    "# for bias due to weights being random however).\n",
    "# Instead we fill them with random noise that has been very carefully selected to work well\n",
    "# for the specific layer structure/non-linearity (see above for reference).\n",
    "# Note that disabling the gradient avoids wasting the memory taken up by the data structure\n",
    "# it builds in anticipation of calculating gradients. In a function so it can be reused below.\n",
    "def init():\n",
    "    with torch.no_grad():\n",
    "        for name in [('w',1), ('w',2), ('w',3)]:\n",
    "            theta[name][...] = torch.randn(size=theta[name].shape) * numpy.sqrt(2 / theta[name].shape[0])\n",
    "\n",
    "init()\n",
    "\n",
    "\n",
    "def leakyReLU(v):\n",
    "    \"\"\"Implementation of a leaky ReLU, just because:-P\"\"\"\n",
    "    return torch.maximum(torch.tensor(0), v) + 0.01 * torch.minimum(torch.tensor(0), v)\n",
    "\n",
    "\n",
    "def model(x, theta):\n",
    "    \"\"\"This should implement the neural network function, taking in x and returning y.\n",
    "    It also takes a dictionary of parameters, called theta.\"\"\"\n",
    "    \n",
    "    # **************************************************************** 2 marks\n",
    "    d = 0.25\n",
    "    k = torch.bernoulli(torch.full((256,),1-d))\n",
    "    v = leakyReLU(x@theta['w',1]+theta['b',1])*k\n",
    "    for i in range(2,4):\n",
    "        v = leakyReLU(v@theta['w',i]+theta['b',i])\n",
    "    return v.squeeze()\n",
    "\n",
    "\n",
    "# A quick test of the model - weights are obviously wrong, so answer will be random,\n",
    "# but checks it runs without error...\n",
    "# (Ignore grad_fn - it's the link to the computation graph, as used for calculating gradients)\n",
    "print('Zero:', model(torch.zeros(20), theta)) # bias is zero so this should return 0!\n",
    "print('Random:', model(torch.rand(20), theta)) # Single random value; if return is 1D you need to .squeeze()!\n",
    "print('Block:', model(torch.rand((7,20)), theta)) # Vectorisation test; if return is 2D you need to .squeeze()!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Realistic training\n",
    "\n",
    "We want to obtain a fair comparison with both the baseline and Thompson sampling from earlier, so we can see how this approach performs relative to them both.\n",
    "But the neural network won't work without training data, and it wouldn't be fair to give it any before the customers arrive!\n",
    "Instead, we take a batch learning approach: random assignment for the first 50% of customers, then use them (with their random assignments) as the training data for the neural network, before making the decision for the remaining 50% of customers with the trained network.\n",
    "This is fair because the network gets the same data in the same order, as long as we report the overall performance for 100% of customers (both those who got random assignment and those who got a decision from the network).\n",
    "We'll also report the performance on the last 50% only however, to get a sense of where the network would end up in a fairground that's open for longer, but note that this number is not comparable to the others.\n",
    "\n",
    "To train the network we need a _cost function_.\n",
    "For this first version we're just going to use root mean squared error (RMSE),\n",
    "\n",
    "$$\\sqrt{\\sum_i \\left( y_i - f(x_i, \\theta) \\right)^2}$$\n",
    "\n",
    "where $f(x, \\theta)$ is the neural network.\n",
    "This doesn't work if using the network within Thompson sampling, but it works fine for the batch approach being taken so we'll make the necesary change in Q3.\n",
    "\n",
    "The batch learning code is provided below, including the code to train the network with the above cost function, so the above is entirely for explanation.\n",
    "Your task is to finish the function that selects the ride for each vistor, that is only run after the network has been trained.\n",
    "However, note that the neural network is noisy due to dropout.\n",
    "This means that to evaluate how good a sample is you must run the network many times, and then generate a summary of the samples; in this case you must take the mean.\n",
    "The `pick_nn()` function includes a samples parameter, which is how many samples per ride to do.\n",
    "\n",
    "Notes:\n",
    "* This will be very slow if you call `model()` for every sample and ride combination. You'll want to use broadcasting in other words!\n",
    "* You should use `with torch.no_grad():` as the gradient isn't required and that will make it faster still.\n",
    "* Remember that the `model()` function takes the customers vector concatenated to the rides vector. Many ways of doing this, but [`cat()`](https://pytorch.org/docs/stable/generated/torch.cat.html) is one choice. [`repeat()`](https://pytorch.org/docs/stable/generated/torch.Tensor.repeat.html) and broadcasting is another.\n",
    "* You can find the ride vectors in the `rides` array, which is indexed `[ride, feature]`. Be aware that it's a `numpy` array, not a `torch` `Tensor`, so conversion has to happen, though most solutions will do so automatically.\n",
    "* This takes about 16 seconds using 32 cores running at 4.5Ghz. It will take some time, but it should remain reasonable even on slower computers. I hope! You can run with a reduced epoch count if you want.\n",
    "* This won't always beat Thompson sampling (above), because it's being stupid for the first half of the customers. Should beat Thompson sampling for the second half however!\n",
    "\n",
    "__(2 marks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 44.7 seconds\n",
      "Training curve:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch neural network ride assignment:\n",
      "    52.5% of customers happy\n",
      "    75.1% of nn customers happy\n"
     ]
    }
   ],
   "source": [
    "def pick_nn(visitor, theta, samples = 16):\n",
    "    \"\"\"Given a 1D feature vector representing a customer this must return the\n",
    "    index of the best ride for that customer. samples is the number of samples\n",
    "    it should take to determine this.\"\"\"\n",
    "\n",
    "    # **************************************************************** 2 marks\n",
    "    with torch.no_grad():\n",
    "        x = torch.cat((visitor.expand(rides.shape[0],visitor.shape[0]),torch.from_numpy(rides)),\n",
    "                       dim=1).unsqueeze(0).repeat([samples]+[1]*2).to(torch.float32)\n",
    "        return torch.argmax(model(x,theta).mean(dim=0))\n",
    "\n",
    "\n",
    "# Build the training set by handling the first 50% of customers randomly...\n",
    "half = visitors.shape[0] // 2\n",
    "\n",
    "which = rng.choice(rides.shape[0], size=half)\n",
    "\n",
    "train_x = numpy.empty((half, 20))\n",
    "train_x[:,:10] = visitors[:half,:]\n",
    "train_x[:,10:] = rides[which,:]\n",
    "\n",
    "train_y = fairground.happy(train_x, rng)\n",
    "\n",
    "     \n",
    "# Train neural network using stochastic gradient descent...\n",
    "epochs = 128\n",
    "batch_size = 64\n",
    "history = []\n",
    "\n",
    "init()\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    # Create a random ordering for assigning visitors to mini-batches...\n",
    "    order = torch.randperm(half)\n",
    "    \n",
    "    # Process each mini-batch...\n",
    "    losses = []\n",
    "    for base in range(0, half, batch_size):\n",
    "        batch_x = torch.tensor(train_x[order[base:base+batch_size],:]).float()\n",
    "        batch_y = torch.tensor(train_y[order[base:base+batch_size]]).float()\n",
    "        \n",
    "        # Reset gradients (if they exist)...\n",
    "        for param in theta.values():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "        \n",
    "        # Calculate loss (rmse)...\n",
    "        pred_y = model(batch_x, theta)\n",
    "        loss = torch.sqrt(torch.square(batch_y - pred_y).mean())\n",
    "        \n",
    "        losses.append(loss.detach())\n",
    "        \n",
    "        # Backpropagation...\n",
    "        loss.backward()\n",
    "    \n",
    "        # Take a step with gradient descent...\n",
    "        # (PyTorch does of course include libraries to do this, but want\n",
    "        # to show what they actually do, even though this is super basic\n",
    "        # and a terrible choice)\n",
    "        step_size = 0.5 * (epochs - epoch) / epochs # This is a simple learning schedule\n",
    "        with torch.no_grad():\n",
    "            for param in theta.values():\n",
    "                param -= step_size * param.grad\n",
    "        \n",
    "    # Record performance (should really calculate, rather than taking the\n",
    "    # average of the minibatches, as they are out of date, but this\n",
    "    # gets a reasonable approximation quickly. Should also have a validation\n",
    "    # set, but that's awkward in this scenario)...\n",
    "    history.append(numpy.array(losses).mean())\n",
    "\n",
    "end_time = time.time()\n",
    "print('Took {:.1f} seconds'.format(end_time - start_time))\n",
    "\n",
    "\n",
    "# Generate a graph of iteration vs performance for nn...\n",
    "print('Training curve:')\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('root mean squared error')\n",
    "\n",
    "plt.plot(numpy.arange(len(history)) + 1, history)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Do the remaining 50% of customers using the network...\n",
    "happy = numpy.empty(visitors.shape[0] - half, dtype=int)\n",
    "\n",
    "for i in range(happy.shape[0]):\n",
    "    choice = pick_nn(torch.tensor(visitors[half+i,:]), theta)\n",
    "    happy[i] = fairground.happy(numpy.concatenate((visitors[half+i,:], rides[choice,:])), rng)\n",
    "\n",
    "\n",
    "# Report performance...\n",
    "print('Batch neural network ride assignment:')\n",
    "print('    {:.1f}% of customers happy'.format(100 * (train_y.sum() + happy.sum()) / visitors.shape[0]))\n",
    "print('    {:.1f}% of nn customers happy'.format(100 * happy.sum() / happy.shape[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
